{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\roshan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\roshan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\roshan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\roshan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# üõ†Ô∏è Download required data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ú® Cleaned Words: ['hey', 'roshan', 'building', 'llm', 'scratch', 'fire']\n"
     ]
    }
   ],
   "source": [
    "# testing word token extraction\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "text = \"Hey Roshan! You're building your own LLM from scratch. That's fire! üî•\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(w) for w in filtered]\n",
    "\n",
    "print(\"‚ú® Cleaned Words:\", lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Detected intent: question\n",
      " Clean tokens detected: ['tell', 'me', 'about', 'agentic', 'ai']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "training_data = [\n",
    "    (\"hello there\", \"greeting\"),\n",
    "    (\"hi bro\", \"greeting\"),\n",
    "    (\"how are you?\", \"greeting\"),\n",
    "    (\"what is NLP?\", \"question\"),\n",
    "    (\"tell me about GPT\", \"question\"),\n",
    "    (\"bye for now\", \"farewell\"),\n",
    "    (\"see you\", \"farewell\"),\n",
    "    (\"yourself\",\"askingme\"),\n",
    "    (\"you\",\"askingme\"),\n",
    "    (\"your\",\"askingme\"),\n",
    "    (\"who are you\",\"askingme\")\n",
    "]\n",
    "\n",
    "#  Preprocess returns both features and clean tokens\n",
    "def preprocess_with_tokens(sentence):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    clean_tokens = [word for word in tokens if word.isalnum()]\n",
    "    features = {word: True for word in clean_tokens}\n",
    "    return features, clean_tokens\n",
    "\n",
    "#  Training\n",
    "train_set = []\n",
    "for text, label in training_data:\n",
    "    features, _ = preprocess_with_tokens(text)\n",
    "    train_set.append((features, label))\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "#  Testing\n",
    "test_input = \"tell me about agentic ai\"\n",
    "features, clean_tokens = preprocess_with_tokens(test_input)\n",
    "label = classifier.classify(features)\n",
    "\n",
    "print(\" Detected intent:\", label)\n",
    "print(\" Clean tokens detected:\", clean_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Bot: That's a deep one... ü§î\n"
     ]
    }
   ],
   "source": [
    "responses = {\n",
    "    \"greeting\": [\"Heyy!\", \"What's up bhai!\", \"Hello legend! üòé\"],\n",
    "    \"question\": [\"Let's learn together!\", \"That's a deep one... ü§î\", \"Here‚Äôs what I know...\"],\n",
    "    \"farewell\": [\"Bye bro!\", \"Catch you later!\", \"Peace out! ‚úåÔ∏è\"],\n",
    "    \"askingme\": [\"im an LLM!\", \"im an ai\", \"im a program\"]\n",
    "}\n",
    "\n",
    "from random import choice\n",
    "reply = choice(responses.get(label,[\"i dont know that yet\"]))\n",
    "print(\"ü§ñ Bot:\", reply)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
